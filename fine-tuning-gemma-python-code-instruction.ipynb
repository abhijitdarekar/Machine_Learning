{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7034934,"sourceType":"datasetVersion","datasetId":4046999},{"sourceId":7087735,"sourceType":"datasetVersion","datasetId":4083841},{"sourceId":11382,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abhijitdarekar001/fine-tuning-gemma-python-code-instruction-and-qa?scriptVersionId=165514795\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### Table of Contents:\n \n1. Installing Libraries & Loading Libraries\n2. Loading and Pre-Processing Data<br>\n3. Loading Gemma <br>\n   a. What should be the value of target modules?<br>\n   b. Initilizing LoRA<br>\n   c. Training Model<br>\n   d. Saving Model<br>\n   e. Load and Merge Model Weights<br> \n4. Text Generation Using LoRA Model\n5. Conclusion\n\n\n#### Dataset Used\n- [Glaive Python Code QA Dataset](https://www.kaggle.com/datasets/thedevastator/glaive-python-code-qa-dataset): This dataset has  common questions about the Python programming language and other programming languages also.\n- [Python Code Instruction](https://www.kaggle.com/datasets/thedevastator/python-code-instruction-dataset) : Dataset consisting of corresponding instruction, input, output, and prompt information of Python Programming Language.\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Installing and Loading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U peft evaluate transformers==4.38.0 accelerate==0.27.2 bitsandbytes==0.42.0 \n!pip  install -U -q trl==0.7.10 datasets==2.17.1\n# %pip  install -q -U rouge_score","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments,EvalPrediction,pipeline\nfrom peft import LoraConfig, TaskType, get_peft_model , PeftModel, LoraModel\nfrom accelerate.utils import release_memory\nfrom accelerate import Accelerator\nfrom datasets import Dataset\nimport evaluate\nimport accelerate\n\nfrom sklearn.metrics import f1_score\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\n\nfrom trl import SFTTrainer\n\nimport torch\nimport torch.nn\n\nimport os\nos.environ['TOKENIZERS_PARALLELISM'] = \"FALSE\"\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport wandb\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:33:38.209965Z","iopub.execute_input":"2024-03-05T06:33:38.21024Z","iopub.status.idle":"2024-03-05T06:33:47.439782Z","shell.execute_reply.started":"2024-03-05T06:33:38.210217Z","shell.execute_reply":"2024-03-05T06:33:47.438854Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-05 06:33:42.907206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-05 06:33:42.907259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-05 06:33:42.908688: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"60"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init() ","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:35:21.273806Z","iopub.execute_input":"2024-03-05T06:35:21.274302Z","iopub.status.idle":"2024-03-05T06:35:55.882938Z","shell.execute_reply.started":"2024-03-05T06:35:21.274264Z","shell.execute_reply":"2024-03-05T06:35:55.882051Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:xzdn8tml) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.047 MB of 0.047 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">firm-pond-4</strong> at: <a href='https://wandb.ai/abhijit-darekar/uncategorized/runs/xzdn8tml' target=\"_blank\">https://wandb.ai/abhijit-darekar/uncategorized/runs/xzdn8tml</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240305_063349-xzdn8tml/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:xzdn8tml). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240305_063521-cfjl1hjx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abhijit-darekar/uncategorized/runs/cfjl1hjx' target=\"_blank\">lunar-brook-5</a></strong> to <a href='https://wandb.ai/abhijit-darekar/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abhijit-darekar/uncategorized' target=\"_blank\">https://wandb.ai/abhijit-darekar/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abhijit-darekar/uncategorized/runs/cfjl1hjx' target=\"_blank\">https://wandb.ai/abhijit-darekar/uncategorized/runs/cfjl1hjx</a>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abhijit-darekar/uncategorized/runs/cfjl1hjx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x794757b876d0>"},"metadata":{}}]},{"cell_type":"markdown","source":" # 2. Loading & Pre-Processing Data","metadata":{}},{"cell_type":"code","source":"code = pd.read_csv(\"/kaggle/input/python-code-instruction-dataset/train.csv\")\ncode = code.sample(frac = 0.1,random_state = 101,ignore_index=True)\nqa = pd.read_csv(\"/kaggle/input/glaive-python-code-qa-dataset/train.csv\",names = ['output','instruction'])\nqa = qa.sample(frac = 0.01,random_state = 101,ignore_index=True)\n\ncode.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:36:36.024407Z","iopub.execute_input":"2024-03-05T06:36:36.025061Z","iopub.status.idle":"2024-03-05T06:36:38.599936Z","shell.execute_reply.started":"2024-03-05T06:36:36.025029Z","shell.execute_reply":"2024-03-05T06:36:38.599025Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1861 entries, 0 to 1860\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   instruction  1861 non-null   object\n 1   input        1307 non-null   object\n 2   output       1861 non-null   object\n 3   prompt       1861 non-null   object\ndtypes: object(4)\nmemory usage: 58.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_data(example):\n    template = f\"\"\"Instruction:\\n{example['instruction']}\\n\\nResponse:\\n{example['output']}\"\"\"\n    return template\n    \ncode['template'] = code.apply(preprocess_data,axis=1)\nqa['template'] = qa.apply(preprocess_data,axis=1)\n\ndataset = Dataset.from_pandas(pd.concat([code[['template']],qa[['template']]])).train_test_split(test_size = 0.2)\n\n# Example Printing\nprint(code['template'].iloc[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:36:38.601634Z","iopub.execute_input":"2024-03-05T06:36:38.601946Z","iopub.status.idle":"2024-03-05T06:36:38.694422Z","shell.execute_reply.started":"2024-03-05T06:36:38.601917Z","shell.execute_reply":"2024-03-05T06:36:38.693455Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Instruction:\nWrite a Python script to print all prime numbers between two numbers, a and b (where a<b).\n\nResponse:\nfor num in range(a, b+1): \n   if num > 1: \n       for i in range(2, num): \n           if (num % i) == 0: \n               break\n       else: \n           print(num)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above is a sample template from created dataset.","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading Gemma","metadata":{}},{"cell_type":"markdown","source":"#### What should be the value of target modules?","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:00:17.356558Z","iopub.execute_input":"2024-03-04T11:00:17.356931Z","iopub.status.idle":"2024-03-04T11:00:17.366376Z","shell.execute_reply.started":"2024-03-04T11:00:17.356903Z","shell.execute_reply":"2024-03-04T11:00:17.365507Z"}}},{"cell_type":"code","source":"model_structure = \"\"\"This is the structure of Gemma Model,\n\n\nGemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\"\"\"\nprint(model_structure)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-05T06:36:38.695471Z","iopub.execute_input":"2024-03-05T06:36:38.695743Z","iopub.status.idle":"2024-03-05T06:36:38.702592Z","shell.execute_reply.started":"2024-03-05T06:36:38.695719Z","shell.execute_reply":"2024-03-05T06:36:38.701615Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"This is the structure of Gemma Model,\n\n\nGemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"When you print model Summary, you will see someting like **Model Attention Layer** in this case you see **GemmaSdpaAttention** which is attention layer of the model.<br>\nIn our case we have attention layers which are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `rotary_emb`, `up_proj`, `gate_proj`, `down_proj`.<br>\nRefer : [target-modules-for-applying-peft-lora-on-different-models](https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models)\n\n\nIn case of LoRA, we have to mention target_modules when defining config for taining a model.\nHere are possible values of `target_modeuls` \n1. `target_modules = \"all-linear\"`<br>\n    This is a convenient shortcut that tells the system to target all linear layers within the pre-trained model for adaptation. This includes the modules you saw previously like `q_proj`, `k_proj`, `v_proj`, `o_proj`,`up_proj`, and `down_proj`.<br>\n    Apart from these, Embedding layers and Output projection layer,\n    \n     - Benfits :<br>\n     a. Simplicity  : it is sraightforward way to adapt a large portion of the model, potentially achieving significant memory savings.<br>\n     b. Potential Performance Gains : As we are including majority of the model, this can help model to learn more efficintly which leads to better fine-tuning performanc.\n     \n     - Cons :<br>\n     a. Over-adaptation : Adapting all linear layers can be overly aggressive, potentially leading to a loss of information from the pre-trained model and hindering performance. <br>\n     b. Increased Memory Foot print : While aiming for memory efficiency, adapting all linear layers will still require some memory overhead compared to targeting specific modules.<br>\n     \n2. `target_modeules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"]`<br>\n    To over come the over-fitting we can secifying individual modules, you can explicitly list the modules you want to adapt for more control. This allows you to focus on specific areas while leaving less relevant parts untouched.<br>\n    Below is the modules desciption.<br>\n     - `q_proj`,`k_porj`,`v_proj` are query, key, values in attention mechinism.\n     - `gate_proj` Projection layer for the gate in the attention mechanism.\n     - `up_proj`, `down_proj` are Projection layers used for the feed-forward network within a transformer block.\n    ","metadata":{}},{"cell_type":"markdown","source":"## Initilizing LoRA","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/gemma/transformers/2b-it/2\"\nlora_config = LoraConfig(\n        lora_dropout=0.1,\n        r=8,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n)\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='fp4',\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config)\n\nloraModel = LoraModel(model= model,config =lora_config,adapter_name = \"adapter\")","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:36:38.704577Z","iopub.execute_input":"2024-03-05T06:36:38.704881Z","iopub.status.idle":"2024-03-05T06:36:45.957318Z","shell.execute_reply.started":"2024-03-05T06:36:38.704856Z","shell.execute_reply":"2024-03-05T06:36:45.956435Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4439fbe22f1f4e1695dd447b1e8bc02e"}},"metadata":{}}]},{"cell_type":"markdown","source":"##### Question : What is number of  trainable parameters in Model?","metadata":{}},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\nprint_trainable_parameters(loraModel)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:36:45.958545Z","iopub.execute_input":"2024-03-05T06:36:45.95903Z","iopub.status.idle":"2024-03-05T06:36:45.971026Z","shell.execute_reply.started":"2024-03-05T06:36:45.959003Z","shell.execute_reply":"2024-03-05T06:36:45.970143Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable params: 9805824 || all params: 1525073920 || trainable%: 0.6429736861541767\n","output_type":"stream"}]},{"cell_type":"code","source":"model_output_dir = \"gemma2b\"\ntraining_args = TrainingArguments(\n    output_dir=model_output_dir,                   \n    num_train_epochs=1,                       \n    per_device_train_batch_size=1,           \n    gradient_accumulation_steps=8,      \n    per_device_eval_batch_size = 1,\n    gradient_checkpointing=True,           \n    logging_steps=25,                        \n    learning_rate=1e-4,                       \n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                    \n    max_steps=-1,\n    warmup_ratio=0.03,                        \n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",                             \n    evaluation_strategy=\"steps\",\n    report_to=\"wandb\"\n    )\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset = dataset['test'],\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n    max_seq_length=256,\n    dataset_text_field = 'template',\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-05T06:36:45.972358Z","iopub.execute_input":"2024-03-05T06:36:45.972764Z","iopub.status.idle":"2024-03-05T06:36:49.431673Z","shell.execute_reply.started":"2024-03-05T06:36:45.972731Z","shell.execute_reply":"2024-03-05T06:36:49.430881Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2577 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5978c87cbe404f31a93cc5bc207a57e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/645 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c9c153237d4c18ae7b3854d84cccdb"}},"metadata":{}}]},{"cell_type":"markdown","source":"\n## Training Model","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T06:36:49.432801Z","iopub.execute_input":"2024-03-05T06:36:49.43314Z","iopub.status.idle":"2024-03-05T07:50:18.759077Z","shell.execute_reply.started":"2024-03-05T06:36:49.433112Z","shell.execute_reply":"2024-03-05T07:50:18.758196Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='322' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [322/322 1:13:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.930200</td>\n      <td>1.228644</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.029400</td>\n      <td>1.093736</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.130400</td>\n      <td>1.017548</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.863100</td>\n      <td>0.998809</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.993500</td>\n      <td>0.984391</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.856300</td>\n      <td>0.963941</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.941900</td>\n      <td>0.961933</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.837300</td>\n      <td>0.947427</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.960500</td>\n      <td>0.940101</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.851300</td>\n      <td>0.932984</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.013800</td>\n      <td>0.931079</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.812800</td>\n      <td>0.929348</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=322, training_loss=1.0088226780387926, metrics={'train_runtime': 4408.2414, 'train_samples_per_second': 0.585, 'train_steps_per_second': 0.073, 'total_flos': 5511819935846400.0, 'train_loss': 1.0088226780387926, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saving Adapter","metadata":{}},{"cell_type":"code","source":"lora_adapter_path = \"LoraAdapter\"\ntrainer.model.save_pretrained(lora_adapter_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:51:53.934212Z","iopub.execute_input":"2024-03-05T07:51:53.934586Z","iopub.status.idle":"2024-03-05T07:51:54.193967Z","shell.execute_reply.started":"2024-03-05T07:51:53.934559Z","shell.execute_reply":"2024-03-05T07:51:54.193175Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Release Memory\ntrainer, model, = release_memory(trainer, model)\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:52:09.184019Z","iopub.execute_input":"2024-03-05T07:52:09.184766Z","iopub.status.idle":"2024-03-05T07:52:09.941242Z","shell.execute_reply.started":"2024-03-05T07:52:09.184737Z","shell.execute_reply":"2024-03-05T07:52:09.940096Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Load and Merge Model Weights ","metadata":{}},{"cell_type":"code","source":"final_model = \"Final_Model\"\n# Loading Base Model\nmodel = AutoModelForCausalLM.from_pretrained(base_model,device_map='auto', torch_dtype=torch.float16)\n\n# Lodaing  Base Model with LoRA adapters.\npeft_model = PeftModel.from_pretrained(model,lora_adapter_path,device_map='auto', torch_dtype=torch.float16)\n\n# Merging and Saving Model\nmodel = peft_model.merge_and_unload(progressbar = True)\nmodel.save_pretrained(final_model)\ntokenizer.save_pretrained(final_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:52:27.641767Z","iopub.execute_input":"2024-03-05T07:52:27.642153Z","iopub.status.idle":"2024-03-05T07:52:49.444181Z","shell.execute_reply.started":"2024-03-05T07:52:27.642124Z","shell.execute_reply":"2024-03-05T07:52:49.442917Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827748052bda41b8918f1bc46940490e"}},"metadata":{}},{"name":"stderr","text":"Unloading and merging model: 100%|██████████| 384/384 [00:00<00:00, 4890.31it/s]\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('Final_Model/tokenizer_config.json',\n 'Final_Model/special_tokens_map.json',\n 'Final_Model/tokenizer.model',\n 'Final_Model/added_tokens.json',\n 'Final_Model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model = release_memory(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:52:59.000814Z","iopub.execute_input":"2024-03-05T07:52:59.001704Z","iopub.status.idle":"2024-03-05T07:52:59.361936Z","shell.execute_reply.started":"2024-03-05T07:52:59.001673Z","shell.execute_reply":"2024-03-05T07:52:59.360711Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 4. Text Generation Using LoRA Model","metadata":{}},{"cell_type":"code","source":"peft_pipe = pipeline(\"text-generation\",final_model, model_kwargs={\"torch_dtype\": torch.float16},\n    device_map='auto',\n    max_new_tokens=512)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:53:02.452328Z","iopub.execute_input":"2024-03-05T07:53:02.452726Z","iopub.status.idle":"2024-03-05T07:53:06.162603Z","shell.execute_reply.started":"2024-03-05T07:53:02.452696Z","shell.execute_reply":"2024-03-05T07:53:06.161736Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4543ed72a5ed406dae68814449860b51"}},"metadata":{}}]},{"cell_type":"code","source":"def get_output(question):\n    prompt = f\"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\\n\\nInstruction:\\n{question}\\n\\nResponse:\\n\"\n    out = peft_pipe(prompt,\n     do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True)\n                    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:53:31.551816Z","iopub.execute_input":"2024-03-05T07:53:31.552241Z","iopub.status.idle":"2024-03-05T07:53:31.557658Z","shell.execute_reply.started":"2024-03-05T07:53:31.55221Z","shell.execute_reply":"2024-03-05T07:53:31.556748Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"out = get_output(\"Write program to find factorial of number\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:53:32.594006Z","iopub.execute_input":"2024-03-05T07:53:32.594955Z","iopub.status.idle":"2024-03-05T07:53:35.288807Z","shell.execute_reply.started":"2024-03-05T07:53:32.594908Z","shell.execute_reply":"2024-03-05T07:53:35.287909Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nWrite program to find factorial of number\n\nResponse:\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\n","output_type":"stream"}]},{"cell_type":"code","source":"out = get_output(\"Explain how can an AI classify a given sentence as a declaration, an expression, or a statement?\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:54:40.039021Z","iopub.execute_input":"2024-03-05T07:54:40.039391Z","iopub.status.idle":"2024-03-05T07:54:41.117892Z","shell.execute_reply.started":"2024-03-05T07:54:40.039365Z","shell.execute_reply":"2024-03-05T07:54:41.116983Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nExplain how can an AI classify a given sentence as a declaration, an expression, or a statement?\n\nResponse:\nAn AI can classify a given sentence as a declaration, an expression, or a statement by analyzing the context and the presence of certain keywords and grammatical structures.\n","output_type":"stream"}]},{"cell_type":"code","source":"out = get_output(\"I need to write a bash script that compares two arguments and prints a message if the first argument is greater than the second argument. Can someone help me with the code\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:55:09.769288Z","iopub.execute_input":"2024-03-05T07:55:09.770179Z","iopub.status.idle":"2024-03-05T07:55:11.04795Z","shell.execute_reply.started":"2024-03-05T07:55:09.770135Z","shell.execute_reply":"2024-03-05T07:55:11.046864Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nI need to write a bash script that compares two arguments and prints a message if the first argument is greater than the second argument. Can someone help me with the code\n\nResponse:\n```bash\n#!/bin/bash\n\nif [[ \"$1\" > \"$2\" ]]; then\n  echo \"The first argument is greater than the second argument.\"\nfi\n```\n","output_type":"stream"}]},{"cell_type":"code","source":"out = get_output(\"How can I optimize a TensorFlow neural network to improve its predictive accuracy?\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-05T07:55:48.241559Z","iopub.execute_input":"2024-03-05T07:55:48.242292Z","iopub.status.idle":"2024-03-05T07:55:52.777381Z","shell.execute_reply.started":"2024-03-05T07:55:48.242262Z","shell.execute_reply":"2024-03-05T07:55:52.776271Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nHow can I optimize a TensorFlow neural network to improve its predictive accuracy?\n\nResponse:\n1. **Data Preparation:** Clean and normalize the data to ensure consistent representation.\n2. **Model Optimization:** Choose an appropriate model architecture and hyperparameters to achieve optimal performance.\n3. **Training and Evaluation:** Implement a robust training loop with early stopping and validation to prevent overfitting.\n4. **Regularization:** Introduce regularization techniques to prevent overfitting and improvegeneralizability.\n5. **Feature Engineering:** Explore feature engineering techniques to extract more relevant features from the data.\n6. **Hyperparameter Tuning:** Optimize the model's hyperparameters to find the best settings for the task.\n7. **Ensemble Methods:** Combine multiple models to leverage their strengths and improve predictive accuracy.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Conclusion \n\nIn this notebook we have fine-tuned **Gemma-2b-it** model from **HuggingFace**  for Python-Code Generation.\n\nWe have covered following topics.\n- Dataset preprocessing.\n- Defining Lora-config for model. \n- Why Choosing correct target Modules are nesseary?.\n- Saving and Merging Lora Adapters with Model.\n- Text Generation for Lora Mode\n","metadata":{}},{"cell_type":"markdown","source":"# Thank you.\n### Please do suggest if there is any corrections related to approach,design, preprocessing and model building.","metadata":{}}]}
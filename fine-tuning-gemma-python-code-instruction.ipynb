{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7034934,"sourceType":"datasetVersion","datasetId":4046999},{"sourceId":7087735,"sourceType":"datasetVersion","datasetId":4083841},{"sourceId":11382,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Table of Contents:\n \n1. Installing Libraries & Loading Libraries\n2. Loading and Pre-Processing Data<br>\n3. Loading Gemma <br>\n   a. What should be the value of target modules?<br>\n   b. Initilizing LoRA<br>\n   c. Training Model<br>\n   d. Saving Model<br>\n   e. Load and Merge Model Weights<br> \n4. Text Generation Using LoRA Model\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Installing and Loading Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q peft trl==0.7.10 evaluate transformers==4.38.0 accelerate==0.27.2 bitsandbytes==0.42.0 datasets==2.18.0","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments,EvalPrediction,pipeline\nfrom peft import LoraConfig, TaskType, get_peft_model , PeftModel\nfrom accelerate.utils import release_memory\nfrom accelerate import Accelerator\nfrom datasets import Dataset\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\n\nfrom trl import SFTTrainer, setup_chat_format\n\nimport torch\nimport torch.nn\n\nimport os\nos.environ['TOKENIZERS_PARALLELISM'] = \"FALSE\"\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:41:25.847963Z","iopub.execute_input":"2024-03-04T12:41:25.848811Z","iopub.status.idle":"2024-03-04T12:41:26.344121Z","shell.execute_reply.started":"2024-03-04T12:41:25.848777Z","shell.execute_reply":"2024-03-04T12:41:26.342989Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"522"},"metadata":{}}]},{"cell_type":"markdown","source":" # 2. Loading & Pre-Processing Data","metadata":{}},{"cell_type":"code","source":"code = pd.read_csv(\"/kaggle/input/python-code-instruction-dataset/train.csv\")\ncode = code.sample(frac = 0.1,random_state = 101,ignore_index=True)\ncode.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:40:41.527246Z","iopub.execute_input":"2024-03-04T12:40:41.527884Z","iopub.status.idle":"2024-03-04T12:40:41.837486Z","shell.execute_reply.started":"2024-03-04T12:40:41.527854Z","shell.execute_reply":"2024-03-04T12:40:41.836523Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1861 entries, 0 to 1860\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   instruction  1861 non-null   object\n 1   input        1307 non-null   object\n 2   output       1861 non-null   object\n 3   prompt       1861 non-null   object\ndtypes: object(4)\nmemory usage: 58.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"code.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:41:30.958227Z","iopub.execute_input":"2024-03-04T12:41:30.959061Z","iopub.status.idle":"2024-03-04T12:41:30.970915Z","shell.execute_reply.started":"2024-03-04T12:41:30.959020Z","shell.execute_reply":"2024-03-04T12:41:30.969958Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                                              instruction  \\\n0  Edit the following Python code to give the output as [400, 125, 10,1].   \n\n                                                                   input  \\\n0  val = [1, 10, 125, 400]\\nres = []\\n\\nfor v in val:\\n    res.append(v)   \n\n                                                                                                                         output  \\\n0  val = [1, 10, 125, 400]\\nres = []\\n\\nfor v in reversed(val):\\n    res.append(v)\\n    \\nprint(res) # Output: [400, 125, 10,1]   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                    prompt  \n0  Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEdit the following Python code to give the output as [400, 125, 10,1].\\n\\n### Input:\\nval = [1, 10, 125, 400]\\nres = []\\n\\nfor v in val:\\n    res.append(v)\\n\\n### Output:\\nval = [1, 10, 125, 400]\\nres = []\\n\\nfor v in reversed(val):\\n    res.append(v)\\n    \\nprint(res) # Output: [400, 125, 10,1]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Edit the following Python code to give the output as [400, 125, 10,1].</td>\n      <td>val = [1, 10, 125, 400]\\nres = []\\n\\nfor v in val:\\n    res.append(v)</td>\n      <td>val = [1, 10, 125, 400]\\nres = []\\n\\nfor v in reversed(val):\\n    res.append(v)\\n    \\nprint(res) # Output: [400, 125, 10,1]</td>\n      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEdit the following Python code to give the output as [400, 125, 10,1].\\n\\n### Input:\\nval = [1, 10, 125, 400]\\nres = []\\n\\nfor v in val:\\n    res.append(v)\\n\\n### Output:\\nval = [1, 10, 125, 400]\\nres = []\\n\\nfor v in reversed(val):\\n    res.append(v)\\n    \\nprint(res) # Output: [400, 125, 10,1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_data(example):\n    template = f\"\"\"Instruction:\\n{example['instruction']}\\n\\nResponse:\\n{example['output']}\"\"\"\n    return template\n    \ncode['template'] = code.apply(preprocess_data,axis=1)\ndataset = Dataset.from_pandas(code[['template']]).train_test_split(test_size = 0.2)\n\n# Example Printing\nprint(code['template'].iloc[1])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:59:54.809533Z","iopub.execute_input":"2024-03-04T11:59:54.809843Z","iopub.status.idle":"2024-03-04T11:59:54.865799Z","shell.execute_reply.started":"2024-03-04T11:59:54.809806Z","shell.execute_reply":"2024-03-04T11:59:54.864924Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Instruction:\nWrite a Python script to print all prime numbers between two numbers, a and b (where a<b).\n\nResponse:\nfor num in range(a, b+1): \n   if num > 1: \n       for i in range(2, num): \n           if (num % i) == 0: \n               break\n       else: \n           print(num)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above is a sample template from created dataset.","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading Gemma","metadata":{}},{"cell_type":"markdown","source":"## What should be the value of target modules?","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:00:17.356558Z","iopub.execute_input":"2024-03-04T11:00:17.356931Z","iopub.status.idle":"2024-03-04T11:00:17.366376Z","shell.execute_reply.started":"2024-03-04T11:00:17.356903Z","shell.execute_reply":"2024-03-04T11:00:17.365507Z"}}},{"cell_type":"raw","source":"This is the structure of Gemma Model,\n\n\nGemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)","metadata":{}},{"cell_type":"markdown","source":"When you print model Summary, you will see someting like **Model Attention Layer** in this case you see **GemmaSdpaAttention** which is attention layer of the model.<br>\nIn our case we have attention layers which are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `rotary_emb`, `up_proj`, `gate_proj`, `down_proj`.<br>\nRefer : [target-modules-for-applying-peft-lora-on-different-models](https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models)\n\n\nIn case of LoRA, we have to mention target_modules when defining config for taining a model.\nHere are possible values of `target_modeuls` \n1. `target_modules = \"all-linear\"`<br>\n    This is a convenient shortcut that tells the system to target all linear layers within the pre-trained model for adaptation. This includes the modules you saw previously like `q_proj`, `k_proj`, `v_proj`, `o_proj`,`up_proj`, and `down_proj`.<br>\n    Apart from these, Embedding layers and Output projection layer,\n    \n     - Benfits :<br>\n     a. Simplicity  : it is sraightforward way to adapt a large portion of the model, potentially achieving significant memory savings.<br>\n     b. Potential Performance Gains : As we are including majority of the model, this can help model to learn more efficintly which leads to better fine-tuning performanc.\n     \n     - Cons :<br>\n     a. Over-adaptation : Adapting all linear layers can be overly aggressive, potentially leading to a loss of information from the pre-trained model and hindering performance. <br>\n     b. Increased Memory Foot print : While aiming for memory efficiency, adapting all linear layers will still require some memory overhead compared to targeting specific modules.<br>\n     \n2. `target_modeules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"]`<br>\n    To over come the over-fitting we can secifying individual modules, you can explicitly list the modules you want to adapt for more control. This allows you to focus on specific areas while leaving less relevant parts untouched.<br>\n    Below is the modules desciption.<br>\n     - `q_proj`,`k_porj`,`v_proj` are query, key, values in attention mechinism.\n     - `gate_proj` Projection layer for the gate in the attention mechanism.\n     - `up_proj`, `down_proj` are Projection layers used for the feed-forward network within a transformer block.\n    ","metadata":{}},{"cell_type":"markdown","source":"## Initilizing LoRA","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/gemma/transformers/2b-it/2\"\nlora_config = LoraConfig(\n        lora_alpha=8, \n        lora_dropout=0.1,\n        r=4,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n)\n\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='fp4',\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:00:48.737062Z","iopub.execute_input":"2024-03-04T12:00:48.737429Z","iopub.status.idle":"2024-03-04T12:00:55.565370Z","shell.execute_reply.started":"2024-03-04T12:00:48.737402Z","shell.execute_reply":"2024-03-04T12:00:55.564400Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c308b6ea597e463c913b8f6386ec8047"}},"metadata":{}}]},{"cell_type":"code","source":"model_output_dir = \"gemma2b\"\ntraining_args = TrainingArguments(\n    output_dir=model_output_dir,                   \n    num_train_epochs=1,                       \n    per_device_train_batch_size=1,           \n    gradient_accumulation_steps=8,      \n    per_device_eval_batch_size = 1,\n    gradient_checkpointing=True,           \n    logging_steps=25,                        \n    learning_rate=2e-4,                       \n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                    \n    max_steps=-1,\n    warmup_ratio=0.03,                        \n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",             \n    report_to=\"tensorboard\",                  \n    evaluation_strategy=\"steps\"          \n    )\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset = dataset['test'],\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    dataset_text_field = 'template',\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:02:27.778530Z","iopub.execute_input":"2024-03-04T12:02:27.779166Z","iopub.status.idle":"2024-03-04T12:02:29.674378Z","shell.execute_reply.started":"2024-03-04T12:02:27.779131Z","shell.execute_reply":"2024-03-04T12:02:29.673490Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1488 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16f3cc195be405682f188db78e3a96c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/373 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1c97a657b346f093ae591a4fbed49b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:02:31.057016Z","iopub.execute_input":"2024-03-04T12:02:31.057655Z","iopub.status.idle":"2024-03-04T12:33:29.456032Z","shell.execute_reply.started":"2024-03-04T12:02:31.057626Z","shell.execute_reply":"2024-03-04T12:33:29.455256Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [186/186 30:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.350300</td>\n      <td>0.989223</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.965100</td>\n      <td>0.919965</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.805600</td>\n      <td>0.877401</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.952300</td>\n      <td>0.863812</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.804900</td>\n      <td>0.849755</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.819700</td>\n      <td>0.839859</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.766800</td>\n      <td>0.836736</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=186, training_loss=0.9232640215145644, metrics={'train_runtime': 1857.5858, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.1, 'total_flos': 2754438242217984.0, 'train_loss': 0.9232640215145644, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saving Adapter","metadata":{}},{"cell_type":"code","source":"lora_adapter_path = \"LoraAdapter\"\ntrainer.model.save_pretrained(lora_adapter_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:33:44.346717Z","iopub.execute_input":"2024-03-04T12:33:44.347433Z","iopub.status.idle":"2024-03-04T12:33:44.478131Z","shell.execute_reply.started":"2024-03-04T12:33:44.347399Z","shell.execute_reply":"2024-03-04T12:33:44.477356Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Release Memory\ntrainer, model, = release_memory(trainer, model)\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:33:52.577060Z","iopub.execute_input":"2024-03-04T12:33:52.578299Z","iopub.status.idle":"2024-03-04T12:33:53.806789Z","shell.execute_reply.started":"2024-03-04T12:33:52.578265Z","shell.execute_reply":"2024-03-04T12:33:53.805612Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Load and Merge Model Weights ","metadata":{}},{"cell_type":"code","source":"final_model = \"Final_Model\"\n# Loading Base Model\nmodel = AutoModelForCausalLM.from_pretrained(base_model,device_map='auto', torch_dtype=torch.float16)\n\n# Lodaing  Base Model with LoRA adapters.\npeft_model = PeftModel.from_pretrained(model,lora_adapter_path,device_map='auto', torch_dtype=torch.float16)\n\n# Merging and Saving Model\nmodel = peft_model.merge_and_unload(progressbar = True)\nmodel.save_pretrained(final_model)\ntokenizer.save_pretrained(final_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:33:59.919926Z","iopub.execute_input":"2024-03-04T12:33:59.920858Z","iopub.status.idle":"2024-03-04T12:34:19.433475Z","shell.execute_reply.started":"2024-03-04T12:33:59.920820Z","shell.execute_reply":"2024-03-04T12:34:19.432459Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9c38190efb4f638e644967f89926f1"}},"metadata":{}},{"name":"stderr","text":"Unloading and merging model: 100%|██████████| 384/384 [00:00<00:00, 4858.36it/s]\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('Final_Model/tokenizer_config.json',\n 'Final_Model/special_tokens_map.json',\n 'Final_Model/tokenizer.model',\n 'Final_Model/added_tokens.json',\n 'Final_Model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model = release_memory(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:34:28.690758Z","iopub.execute_input":"2024-03-04T12:34:28.691359Z","iopub.status.idle":"2024-03-04T12:34:29.203077Z","shell.execute_reply.started":"2024-03-04T12:34:28.691327Z","shell.execute_reply":"2024-03-04T12:34:29.202079Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# 4. Text Generation Using LoRA Model","metadata":{}},{"cell_type":"code","source":"peft_pipe = pipeline(\"text-generation\",final_model, model_kwargs={\"torch_dtype\": torch.float16},\n    device_map='auto',\n    max_new_tokens=512)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:34:33.896613Z","iopub.execute_input":"2024-03-04T12:34:33.896946Z","iopub.status.idle":"2024-03-04T12:34:37.778538Z","shell.execute_reply.started":"2024-03-04T12:34:33.896921Z","shell.execute_reply":"2024-03-04T12:34:37.777548Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb186015a1fc41e3ad642378fc86d043"}},"metadata":{}}]},{"cell_type":"code","source":"def get_output(question):\n    prompt = f\"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\\n\\nInstruction:\\n{question}\\n\\nResponse:\\n\"\n    out = peft_pipe(prompt,\n     do_sample=True,\n    temperature=0.1,\n    top_k=20,\n    top_p=0.3,\n    add_special_tokens=True)\n                    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:38:08.491745Z","iopub.execute_input":"2024-03-04T12:38:08.492657Z","iopub.status.idle":"2024-03-04T12:38:08.497768Z","shell.execute_reply.started":"2024-03-04T12:38:08.492623Z","shell.execute_reply":"2024-03-04T12:38:08.496694Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"out = get_output(\"Write program to find factorial of number\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:38:08.664499Z","iopub.execute_input":"2024-03-04T12:38:08.665144Z","iopub.status.idle":"2024-03-04T12:38:13.057246Z","shell.execute_reply.started":"2024-03-04T12:38:08.665113Z","shell.execute_reply":"2024-03-04T12:38:13.056327Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nWrite program to find factorial of number\n\nResponse:\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\nprint(factorial(5)) # Output: 120\n```\n\nThis code defines a Python function called `factorial` that takes a single integer argument, `n`, and returns the factorial of that number. The base case is when `n` is 0, which returns 1. Otherwise, it recursively calls itself with the argument `n-1` and multiplies the result by `n`. The function is then called with the argument 5, which returns 120.\n","output_type":"stream"}]},{"cell_type":"code","source":"out = get_output(\"write a program to explain simple iterators and generators\")\nprint(out[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T12:38:13.058840Z","iopub.execute_input":"2024-03-04T12:38:13.059131Z","iopub.status.idle":"2024-03-04T12:38:19.152536Z","shell.execute_reply.started":"2024-03-04T12:38:13.059106Z","shell.execute_reply":"2024-03-04T12:38:19.151596Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\n\nInstruction:\nwrite a program to explain simple iterators and generators\n\nResponse:\nSure, here's a simple Python program to explain the concept of iterators and generators:\n\n```python\ndef my_iterator():\n    for i in range(10):\n        yield i\n\nfor item in my_iterator():\n    print(item)\n\n# Output:\n# 0\n# 1\n# 2\n# 3\n# 4\n# 5\n# 6\n# 7\n# 8\n# 9\n```\n\nThis code defines a generator function called `my_iterator` that yields the numbers from 0 to 9. The `for` loop iterates over the generator and prints each item.\n\nThis is a simple example of how generators can be used to create a sequence of values on demand. This can be useful when you need to generate a large number of values or when you need to avoid creating a list or array first.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}